{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Report - Dean Knudson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This semester I continued my work on a sentiment analysis engine. The semester began with the revelation that a new library called transformers, by the HuggingFace, had recently been released. This library made fine-tuning some of the SOTA (state of the art) langauage models incredibly simple. With this, I decided to finetune a variety of BERT architures using the custom dataset I created last semester. At the same time another team was working on revamping the existing media analytics website and a sentiment analysis component was requested from me. This progressed into the construction and deployment of a python machine learning for text classification package. I also briefly experimented with the dataset I used in my [special topic](https://github.com/knuddj1/Sentiment-Analysis/blob/master/recap%20stuff/semester_recap.ipynb), to create a 5 label sentiment analysis model, comparing its results with my custom dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Transformers](https://github.com/huggingface/transformers)\n",
    "\n",
    "The beginning of this semester yielded a new tool in the NLP community.The work I did [last semester](https://github.com/knuddj1/Project-1-Sentiment-Analysis/blob/master/report.ipynb) was made fairly irrelevant by transformers. Created by the HuggingFace team, transformers took all the difficult implementation details out of using the recent, powerful,transformer based, language models and provided a fairly compact python interface for interacting with them. Originally transformers only supported using the popular machine learning framework [PyTorch](https://github.com/huggingface/transformers) as a backend, and was named pytorch_transformers. It now supports tensorflow as well, however I began using the library when only pytorch was available, so all my work is using pytorch. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiarizing myself with Transformers and PyTorch\n",
    "\n",
    "Trasnformers is very simple to use compared to using other existing repos or reimplementing these models from scratch. However, its still not the most straight forward library to use and required quite a few weeks for me to really figure it out. This was partly due to the fact I had little exposure to PyTorch, as I had predominantly been using Keras for my machine learning work.\n",
    "\n",
    "I started off working through PyTorch's [tutorials](pytorch.org/tutorials/), and completed two, that seemed to be most relevant for me.\n",
    "\n",
    "1. ***Writing Custom Datasets, Dataloaders and Transforms*** went in depth on how to load and preprocess/augment data from a non trivial dataset in pytorch fashion. This introduced me to pytorch dataloaders and custom datasets. A pytorch dataset is an object that contains a list of data and implements the `__get__` dunder method, which is used  to extract a single sample given an index. A dataloader is basically a generator function that uses multithreading to extract batchs of samples from a pytorch dataset in parrallel to main thread of execution.\n",
    "\n",
    "2. ***Sequence-to-Sequence Modeling with NN.Transformer and TorchText***, a tutorial on how to train a sequence-to-sequence model that uses the nn.Transformer module. The models featured on HuggingFace's Transformers are made up of layers of these Transformer blocks. I thought that this tutorial would be useful before tackling the Trasformers library as it goes over the entire pipeline of a training a transformer model. Most of this however turned out to be unnecessary, because of Transformer's level of abstraction, and acted more as lesson.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting hands dirty\n",
    "\n",
    "I then moved on to try and learn to use the Transformers library. I started off just wanting to get one of the available models instantiated and perform a forward/backwards pass, for sequence classification. \n",
    "\n",
    "Below is a similar example of how this initial test may have looked:\n",
    "\n",
    "1. **Declare the required imports**\n",
    "   ```\n",
    "        import torch\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "   ```\n",
    "   \n",
    "   - torch is an alias for PyTorch. It is required to use the transformers package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Define model type and load its components**\n",
    "    ```\n",
    "        model_type = 'bert-base-uncased'\n",
    "        tokenizer = BertTokenizer(model_type)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_type)\n",
    "    ```\n",
    "\n",
    "    - model_type defines which version of the model is to be used. \n",
    "    - BertTokenizer is a tool that preprocesses a piece of text into numerical tokens for the model.\n",
    "    - BertForSequenceClassification pretrained BERT transformer model with an untrained classification head connected to the   final hidden layers output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Transform raw text into an input tensor for the model**\n",
    "    ```\n",
    "        input_text = \"Hello, my dog is cute\"\n",
    "        encoded_text = tokenizer.encode(input_text)\n",
    "        input_tensor = torch.tensor(encoded_text).unsequeeze(0) # Add batch dimension\n",
    "        input_label = torch.tensor([1]).unsqueeze(0)  # Add batch dimension\n",
    "    ```\n",
    "    \n",
    "    - input_text is a raw string a of text.\n",
    "    - encoded_text is the output of the tokenizers encode function. It is a list of integers in\n",
    "      the range 0 - Vocabulary size (30000) with a list length of the same number of words in the input_text.\n",
    "    - input_tensor is the encoded_text list converted to a PyTorch tensor.\n",
    "    - input_label is the same as the input_tensor but it is the what will be used as the ground \n",
    "      truth to compare to the models prediction.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Perform a forward pass of the model**\n",
    "    ```\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    ```\n",
    "    \n",
    "    - The model returns a tuple containing the loss (how close its prediction was to the target label)\n",
    "      and logits (a probability distribution of size n, where n is the number of class labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Update weights**\n",
    "    ```    \n",
    "        loss.backward()\n",
    "    ```\n",
    "    \n",
    "    - Performs back propagation using gradient decent and updates the models weights accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full example above, Code only:**\n",
    "\n",
    "    import torch\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        \n",
    "    model_type = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer(model_type)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_type)\n",
    "        \n",
    "    input_text = \"Hello, my dog is cute\"\n",
    "    encoded_text = tokenizer.encode(input_text)\n",
    "    input_tensor = torch.tensor(encoded_text).unsequeeze(0) # Add batch dimension\n",
    "    input_label = torch.tensor([1]).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss, logits = outputs[:2]\n",
    "    loss.backward()\n",
    "    \n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "As seems to be tradition in my machine learning projects, I then decided to create another grid search. This time however, my search was fairly abstract, because I was fine-tuning a model instead of training from scratch. This meant that I didn't need to train multiple versions of the same model with slight tweaks for example: embedding size, number of units in a layer, optimizers and layer activation functions. Instead my grid search consisted of: different model architectures, different pretrained models and batch size. This resulted in a much smaller number of models that needed to be trained and an overall shorter grid search time.\n",
    "\n",
    "The following is a comparison between this grid search and those which I created during my special topic and previous semesters project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Topic\n",
    "\n",
    "---\n",
    "\n",
    "| Hyper Parameter             | Options\n",
    "|-----------------------------|:------------------------------:\n",
    "| Embedding Size              | 50, 100, 150, 200, 250 and 300 \n",
    "| Numbers of Recurrent Layers |                      1, 2 or 3 \n",
    "| Types of Recurrent Layers   |                    GRU or LSTM \n",
    "| Total Unique Models         |                             36 \n",
    "| Training Time               |                        2 Weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Project One</h3></center>\n",
    "\n",
    "---\n",
    "\n",
    "|    Hyper Parameter    |        Options        |\n",
    "|:---------------------:|:---------------------:|\n",
    "| Embedding Size        |           768 or 1024 |\n",
    "| Validation Split Size |            0.1 or 0.2 |\n",
    "| Batch Size            |        64, 128 or 256 |\n",
    "| Optimizers            |  SGD, RMSprop or Adam |\n",
    "| Dropout Rates         |  0.1, 0.2, 0.3 or 0.4 |\n",
    "| Activations           | ReLu, tanh or sigmoid |\n",
    "| Number Dense Layers   |             1, 2 or 3 |\n",
    "| Dense Layer Values    |    32, 64, 128 or 256 |\n",
    "| Total Unique Models   |                 36288 |\n",
    "| Search Time         \t|               6 Days  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<html>\n",
    "    <style type=\"text/css\">\n",
    "    .tg  {border-collapse:collapse;border-spacing:0;}\n",
    "    .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    "    .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    "    .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    "    .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "    .tg .tg-dvpl{border-color:inherit;text-align:right;vertical-align:top}\n",
    "    </style>\n",
    "\n",
    "<table>\n",
    "<tr><th>Table 1 Heading 1 </th><th>Table 1 Heading 2</th></tr>\n",
    "<tr><td>\n",
    "    <table class=\"tg\">\n",
    "      <tr>\n",
    "        <th class=\"tg-c3ow\">Hyper Parameter</th>\n",
    "        <th class=\"tg-c3ow\">Options</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Embedding Size</td>\n",
    "        <td class=\"tg-dvpl\">768 or 1024</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Validation Split Size</td>\n",
    "        <td class=\"tg-dvpl\">0.1 or 0.2</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Batch Size</td>\n",
    "        <td class=\"tg-dvpl\">64, 128 or 256</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Total Unique Models</td>\n",
    "        <td class=\"tg-dvpl\">36</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\"></td>\n",
    "        <td class=\"tg-dvpl\">2 Weeks</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Optimizers</td>\n",
    "        <td class=\"tg-dvpl\">SGD, RMSprop or Adam</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Dropout Rates</td>\n",
    "        <td class=\"tg-dvpl\">0.1, 0.2, 0.3 or 0.4</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Activations</td>\n",
    "        <td class=\"tg-dvpl\">ReLu, tanh or sigmoid</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Number Dense Layers</td>\n",
    "        <td class=\"tg-dvpl\">1, 2 or 3</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Dense Layer Values</td>\n",
    "        <td class=\"tg-dvpl\">32, 64, 128 or 256</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Total Unique Models</td>\n",
    "        <td class=\"tg-dvpl\">36</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Training Time</td>\n",
    "        <td class=\"tg-dvpl\">2 Weeks</td>\n",
    "      </tr>\n",
    "    </table>\n",
    "\n",
    "</td><td>\n",
    "\n",
    "<table class=\"tg\">\n",
    "      <tr>\n",
    "        <th class=\"tg-c3ow\">Hyper Parameter</th>\n",
    "        <th class=\"tg-c3ow\">Options</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Embedding Size</td>\n",
    "        <td class=\"tg-dvpl\">768 or 1024</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Validation Split Size</td>\n",
    "        <td class=\"tg-dvpl\">0.1 or 0.2</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Batch Size</td>\n",
    "        <td class=\"tg-dvpl\">64, 128 or 256</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Total Unique Models</td>\n",
    "        <td class=\"tg-dvpl\">36</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\"></td>\n",
    "        <td class=\"tg-dvpl\">2 Weeks</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Optimizers</td>\n",
    "        <td class=\"tg-dvpl\">SGD, RMSprop or Adam</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Dropout Rates</td>\n",
    "        <td class=\"tg-dvpl\">0.1, 0.2, 0.3 or 0.4</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Activations</td>\n",
    "        <td class=\"tg-dvpl\">ReLu, tanh or sigmoid</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Number Dense Layers</td>\n",
    "        <td class=\"tg-dvpl\">1, 2 or 3</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Dense Layer Values</td>\n",
    "        <td class=\"tg-dvpl\">32, 64, 128 or 256</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Total Unique Models</td>\n",
    "        <td class=\"tg-dvpl\">36</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td class=\"tg-0pky\">Training Time</td>\n",
    "        <td class=\"tg-dvpl\">2 Weeks</td>\n",
    "      </tr>\n",
    "    </table>\n",
    "\n",
    "</td></tr> </table>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
