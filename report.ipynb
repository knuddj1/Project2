{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Report - Dean Knudson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This semester I continued my work on a sentiment analysis engine. The semester began with the revelation that a new library called transformers, by the HuggingFace, had recently been released. This library made fine-tuning some of the SOTA (state of the art) langauage models incredibly simple. With this, I decided to finetune a variety of BERT architures using the custom dataset I created last semester. At the same time another team was working on revamping the existing media analytics website and a sentiment analysis component was requested from me. This progressed into the construction and deployment of a python machine learning for text classification package. I also briefly experimented with the dataset I used in my [special topic](https://github.com/knuddj1/Sentiment-Analysis/blob/master/recap%20stuff/semester_recap.ipynb), to create a 5 label sentiment analysis model, comparing its results with my custom dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Transformers](https://github.com/huggingface/transformers)\n",
    "\n",
    "The beginning of this semester yielded a new tool in the NLP community.The work I did [last semester](https://github.com/knuddj1/Project-1-Sentiment-Analysis/blob/master/report.ipynb) was made fairly irrelevant by transformers. Created by the HuggingFace team, transformers took all the difficult implementation details out of using the recent, powerful,transformer based, language models and provided a fairly compact python interface for interacting with them. Originally transformers only supported using the popular machine learning framework [PyTorch](https://github.com/huggingface/transformers) as a backend, and was named pytorch_transformers. It now supports tensorflow as well, however I began using the library when only pytorch was available, so all my work is using pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiarizing with Transformers and PyTorch\n",
    "\n",
    "Trasnformers is very simple to use compared to using other existing repos or reimplementing these models from scratch. However, its still not the most straight forward library to use and required quite a few weeks for me to really figure it out. This was partly due to the fact I had little exposure to PyTorch, as I had predominantly been using Keras for my machine learning work.\n",
    "\n",
    "I started off training the smallest size of original BERT model, bert-base-uncased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
